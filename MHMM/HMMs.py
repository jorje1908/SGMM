#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jul  9 14:08:17 2019

@author: george

Implements 
--> A hidden Markov Model and extends to
--> a Mixture of Hidden Markov Models,

--> Observational Model is consisted by a Mixture of 
Gaussians Model,

--> The Model will be trained by the Expectation Maximization
    Algorithm 


"""

import numpy as np
from scipy.stats import multivariate_normal
from sklearn.cluster import KMeans

#########     check functions   #########
def checkShape(  arg1, arg2, name):
        
    if arg1.shape != arg2.shape:
        print( "Warning shapes does not match in " + name)
        
        
    return



class HMM(object):
    """  
    HMM class 
    
    implements all the basic HMM algorithms
    in addition to an extension to be used by a 
    Mixture of Hidden Markov Models class MHMM
    
    """
    
    
    
    def __init__(self, states = 2, g_components = 2, t_cov = "diag", 
                 gmm_init = 'Kmeans', kmean_Points = 1000):
        
        #setting covariance type attribute
        self.t_cov = t_cov
        #Setting states attribute
        self.states_ = states
        #Setting Gaussian components attribute
        self.g_components_ = g_components
        #initialize Transition Matrix
        self.A = np.zeros( shape = [states, states], dtype = 'float')
        #initilize initial distribution probabilities np.array pi[k] k = 1...K
        self.pi = np.zeros( shape = states, dtype = 'float')
        
        #4d array cov[k,l,d,d] kth state lth gaussian component
        self.cov = None
    
        #3d array means[k,l,d] kth state lth mean
        self.means = None
    
        #list of gaussian predictors gauss[k,l] lth gaussian of kth state
        #each gauss[k][l] component is an instance of the multivariate_normal
        #scipy object
        self.gauss = None
        #initial probabilities of each gaussian component, for each state
        #alpha[k,l] kth state lth component
        self.alpha = None
        #how to initialize Mixture of Gaussians 
        self.gmm_init = gmm_init
        #how many random points to use for the kmeans
        self.kmean_Points = kmean_Points
        
    def predict_states_All(self, X ):
        """
        computes the probability to observe a x_t for all t = 1...T
        for all states K = 1...K
        for any observation in time it calculates the probability 
        using the method predict_states
        
        X  = (T,d) is a sequence of observations x1, ... xT
        numpy array Txd
        
        return the matrix P[k,t] probability to observe x_t given state k
        
        p(x_t | z_t = k, H = m) for all t for all k
        
        
        """
        
        #get the number of sequneces in time
        T = X.shape[0]
        #get number of states
        K = self.states_
        
        P_all = np.array( shape = [K, T])
        
        for t in np.arange( T ):
            P_all[:, t] = self.predict_states( X[t] )
        
        return P_all
        
    def predict_states(self, x):
        """ 
         calculates the probability of x to be observed given a state,
         for all states and returns a matrix 
         P(x) = [p(x_t|z_t = 1)... p(x_t|z_t = k)]
         
         x = (d, )
        
        """
        K = self.states_
        P = np.zeros( shape = [K] )
        
        for k in np.arange( K ):
            P[k] = self.predict_state( x, st = k)
            
        return P
        
        
    def predict_state(self, x, st = None):
         """
         calculates the probabiity of an observation to be generated 
         by the state "state" --> p(x_t | z_t = k)
         
         returns a matrix kx1
         x = (d,)
         
         """
         
         components = self.g_components_
         pk = 0
         
         for component in np.arange( components ):
             pk += self.predict_state_comp( x, st = st, cmp = component)
             
         return pk
            
         
         
    def predict_state_comp(self, x, st = None, cmp = None):
         
         """ 
         predicts the probability of an observation to be generated
         by the lth component gaussian of the kth state
         
         st: HMM  state {1...K}
         cmp: Kth state lth Gaussian Component l = {1...L}
         
         p( G_t = l, x_t |  z_t = k) = 
         p(x_t | G_t = l, z_t = k)p(G_t = l|z_t = k)
         
         x = (d,)
         
         """
         
         #get the multivariate_gaussian of state = "state"
         #component = "comp"
         gaussian_kl = self.gauss[st][ cmp ]
         #get the probability of this component to be chosen
         alpha_kl = self.alpha[st, cmp]
         #predict the probability of x to be generated by the lth component
         #of
         pkl = gaussian_kl.pdf( x )*alpha_kl
         
         return pkl
     
    def forward(self, X):
        """
        Implements the forward Algorithm, 
        Returns tha matrix forw [ a_1,... a_T ]
        where a_t = [at[1], ... ai[K]]^(transpose)
        where ai[k] = p( z_i = k, x_1...x_t)
        
        X  = (T,d ) is a sequence of observations x1, ... xT
        numpy array Txd

        
        """
       
        #get length of obseravtions in time
        T = X.shape[0]   
        #get number of states
        K = self.states_
        #get transition matrix Aij = p(z(t) = j|z(t-1) = i)
        A = self.A
        #initialize forward matrix
        forw = np.zeros( shape = [K, T])
        
        #take initial alphas a_1
        forw[:,0] = self.predict_states(X[0])*self.pi
        
        for t in np.arange(1, T):
            
            forw[:,t] = self.predict_states( X[t] )*(A.T@ forw[:, t-1] )
            
        return forw
    
    def predict_x( self, X ):
        """
        X = (T,d )is a Txd matrix T observations of d dimension
        predict the probability of to be generated by the model
        p( x | H = m) = p(x_1,...,x_T | H = m)
        
        """
        #take the time 
        T =  X.shape[0] 
        #compute forward probabilities
        forw = self.forward( X )
        
        #sum of aj(T) j = 1 ... K
        pX = np.sum( forw[:, T-1] )
        
        return pX
    
    def backward(self, X):
        """
        Implements the Backward algorithm
        Returns the matrix backw [b_1,..., b_T]
        where b_t = [bt[1],..., bt[K]]
        where bt[k] = p(x_(t+1)...x_T| z_t = k)
        
        X =(T,d) is a sequence of observations x1, ... xT
        numpy array Txd
        
        """
        
        #get length of obseravtions in time
        T = X.shape[0]   
        #get number of states
        K = self.states_
        #get transition matrix Aij = p(z(t) = j|z(t-1) = i)
        A = self.A
        #initialize backward matrix
        backw = np.zeros( shape = [K, T])
        
        #initialize backw matrix at time T
        backw[:, T-1] = 1
        
        for t in np.arange( T-2, -1, -1):
            
            backw[:, t] = A.T@( self.predict_states( X[t+1]*backw[:,t+1]))
            
        return backw
    
    
    def gamas( self, X):
        
        """
        Computes the probability of being at state k at time t given
        we have observed all the sequence x1...xT,
        it is called  smoothed probability on hmms 
        p(z_t = k| O1...OT)
        
        X = (T,d) is a sequence of observations x1, ... xT
        numpy array Txd
        
        returns the gamma matrix  a KxT numpy array K are the HMM states
        T is the length of sequence in time
        
        """
        
        #regularization
        reg = 10**( -6 )
        
        #run forward algorithm_
        forw = self.forward( X )
        #run backward algorithm
        backw = self.backward( X )
        #calculate gamma unnormalized
        gamma = forw*backw 
        gammaSumCol = np.sum( gamma, axis = 0)
        #calculate final gamma
        gamma = gamma/( gammaSumCol + reg )
        
        return gamma


    def sliced( self, X):

        """
        Computes the sliced probability  p(z_t = i, Z_(t+1) = j| o1...T)
        
        returns a KxKxT-1 matrix  xis with the aformentioned probabilities
        
        X = (T, d)
        """
        
        #get length of obseravtions in time
        T = X.shape[0]   
        #get number of states
        K = self.states_
        #get transition matrix Aij = p(z(t) = j|z(t-1) = i)
        A = self.A
        #initialize xis matrix
        xis = np.array( shape = [K, K, T-1 ])
        
        #compute observation proabilities for all time steps of X
        Pkt = self.predict_states_All( X )
        #compute forward
        forw = self.forward( X )
        #compute backward 
        backw = self.backward( X )
        
        for t in np.arange( T-1 ):
            
            xis[:, :, t] = (A.T*forw[:, t]).T*( Pkt[:, t+1]*backw[:, t+1] )
            xisSum = np.sum( xis[:, :, t])
            xis[:,:,t] = xis[:,:,t]/xisSum
            
        return xis
    
    def g(self, x_i):
        """
        calculates the posterior probabilities 
        
        p(G = l|x_t, H = m, z_t = k)
        for all l = {1,...,L}
                t = {1,...,T}
                z_t = {1,...K}
                
        return an KXLXT matrix containing the gkl(t) 
        x_i is a Txd array having the observations in time
        """
        #get the time up to observations reach T
        T = x_i.shape[0]
        #get the number of states
        states = self.states_
        #get the number of gaussian components
        L = self.g_components_
        #initialize matrix
        gs = np.zeros( shape = [states, L, T] )
        
        for state in np.arange( states ):
            gs[state, :, :] = self.g_state( x_i, state )
            
        return gs
            
    
    def g_state(self, x_i, st = None):
        """
        computes the probability p(G_t = l|z_t = k, x_t)
        for state z_t = st and component G_t = comp
        for each time observation o_t t=1 ...T 
        for each component
        thus returning a :
        
        LxT matrix
        
        x_i = [x_i(1),...x_i(T)]
        x_i = ( Txd )
        """
        #number of gaussian components
        gauss_comp = self.g_components_
        #number of observations in time
        T = x_i.shape[0]
        #initialize the matrix to return
        pk = np.zeros( shape = [gauss_comp, T])
        
        for cmp in np.arange( gauss_comp ):
            pk[cmp, :] = self.predict_state_comp(x_i, st = st, cmp = cmp)
        
        #get the component wise sum for each sample in time
        sumPk = np.sum( pk, axis = 0)
        #normalize the pk matrix such that every column sums to 0
        pk = pk/sumPk
        return pk
        
    
    def EM_init(self, X, pi = None, A = None,alpha = None,
                                             means = None, cov = None):
        """
        Initialize the HMM parameters
        
        """
        self.pi_init( pi )
        self.A_init( A )
        self.alpha_means_cov_init(X, alpha, means, cov )
        
    def pi_init(self, pi = None):
        """
        initialize initial state distribution randomly or
        with a custom matrix
        
        """
        
        if pi is not None:
            self.pi = pi
            
        else:
            
            pi = np.random.rand( self.pi.shape[0] )
            pi = pi/np.sum( pi )
            self.pi = pi
            
        return self
        
        
    def A_init(self, A = None):
        """
        Initialize state transition matrix with a custom matrix A
        or randomly
        
        """
        
        if A is not None:
            self.A = A
        
        else:
            A = np.random.rand( self.A.shape[0], self.A.shape[1])
            Asum = np.sum ( A, axis = 1)
            A = A.T/Asum
            self.A = A.T
        
        return self
            
    def alpha_means_cov_init(self, X, alpha, means, cov):
        """
        used by EM_init method
        Initializes alphas means and covs either with 
        custom matrix or with kmeans
        
        X = (N, T, d)
        
        """
        init_type = self.gmm_init
        if (alpha is None) and (cov is None ) and ( means is None ):
            
            if init_type == "Kmeans":
                self.kmeans_init( X )
        
        if (alpha is not None) and (means is not None) and ( cov is not None):
            self.alpha = alpha
            self.means = means
            self.cov = cov
            
        return self
    
    def kmeans_init(self,  X ):
        """
        it is used from 
        "alpha_means_cov_init "
        method
        
        initiializes means with Kmeans
        covariances diagonal with variance 1
        alphas accordingly
        
        X = (N, T, d)
        
        """
        
        #take the number of points to use for Kmeans
        points = self.kmean_Points
        
        #number of points in dataset
        N = len( X ) 
        #number of samples in observation
        T = X[0].shape[0]
        #data dimension 
        d = X[0].shape[1] 
        #gaussian components
        L = self.g_components_
        
        #check points
        if points > N*T:
            points = N*T
            self.kmean_Points = points
        #make the dataset to use in the kmeans
        X_make = self.make_dataset( X, points)
        
        #initialize Means 
        labels = self.kmeans_init_(X_make)
        
        #initialize alphas
        #number of points
        N_x = len( labels )
        #find alphas
        alphaL = np.zeros(L)
        for l in np.arange( L ):
            indxl = len( np.where( labels == l )[0] )
            alphaL[l] = indxl/N_x
        
        self.alpha[:] = alphaL
        
        #initialize Covarinaces
        self.cov[:,:] = np.eye( d )
        
        return self
        
    
    def kmeans_init_(self, X_make):
        """ 
        this function is used by kmeans_init
        Run the kmeans algorithms and sets the clsuter means
        X_make = [self.kmean_points, d]
        """
       
        L = self.g_components_
        kmeans = KMeans( n_clusters = L )
        model = kmeans.fit(X_make)
        means = model.cluster_centers_
        labels = model.labels_
        self.means[:] = means
        
        return labels
        
        
    
    def make_dataset( self, X, points):
        """
        returns a dataset with points number of observations from X
        """
        T = X[0].shape[0]
        N = len( X )
        d = X[0].shape[1] 
        
        #see how many points we need to concatenate together
        indx_num = np.ceil( points/ T )
        #choose random indexes
        indx = np.random.choice( np.arange(N), size = indx_num, replace = False)
        
        #return the Kmeans dataset
        X_kmeans = X[indx]
        X_kmeans = np.reshape( X_kmeans, [-1, d])
        
        return X_kmeans
        
        
        
        
    def EM_iter(self, X, r_m):
        """ 
        
        EM iteration updating all the 
        HMM parameteres
        
        A: state transition matrix
        pi: initial state probabilities
        alpha: gaussian mixing coefficients [k,l] matrix
        means: Gaussian Means [k,l,d] matrix
        cov: Coavriance matrices for the Gaussians [k,l,d,d]
        
        sets the A,pi, alpha, means, cov
        and gauss: gaussian components objects (frozen)
        
        X: is a list of obseravtions O^(1)... O^(N)
        O^(i) = [o_1^(i),....o_T^(i)]
        
        r_m: N dimensional matrix with the posterior probability p(H = m|X)
        which the probability of this HMM to be chosen given the observations
        X
        
        """
        
        K = self.states_
        L = self.g_components_
        
        #feature dimension
        d = X.shape[1]
       
        #initializing attributes needed
        
        self.initialize_EM_sums( K, L, d,  r_m )
        
        for i in np.arange( len(X) ):
            #get the ith observation
            x_i = X[i]
            #get the gammas for the i_th observation KXT
            gamma_i = self.gamas( x_i )
            #get xis for the i_th observation KXKX(T-1)
            xis_i = self.sliced( x_i )
            #get the rm_i,  N 
            rm_i = r_m[i]
            #get g_is for the ith observation KxLXT
            g_i = self.g(x_i)
            
            #update sum of pis
            self.update_pi( gamma_i[:, 0], rm_i )
            #update A matrix nominator and denominator
            self.update_A( xis_i, gamma_i, rm_i )
            #update alphas
            membs_i=self.update_alpha( rm_i, gamma_i, g_i)
            
            self.update_means_cov( x_i, membs_i)
        
        #set all the model parameteres
        self.set_EM_updates()
            
    def initialize_EM_sums( self, K, L, d,  r_m ):
        """
        initializes all the parameteres used in the EM_iter
        to be used in the inside for loop 
        if the dataset is way too big we might need to put 
        the observations in chuncks until we do a full EM update
        
        """
        
        #sum  of initial state distributions
        #for all the K states
        self.pi_Sum = np.zeros( shape = [K] )
        
        #sum of all rm_i's
        self.rm_Sum = np.sum(r_m)
        
        #nominator for state transition probabilities
        self.A_nom = np.zeros( shape = [K,K] )
        
        #A denominator is the same as alpha denominator
        self.A_den = np.zeros( shape = [K] )
        
        #alpha_Nom is the same as means denominator
        #and covarinaces denominator
        #priors of the Gaussian components
        self.alpha_Nom = np.zeros( shape = [K, L] )
        
        #nominator of the means
        self.means_Nom = np.zeros( shape = [K, L, d])
        #nominator of the covariances #need to subtract means
        self.cov_Nom = np.zeros( shape = [K, L, d, d] )
        
        return self
        
    def set_EM_updates(self ) :
        """
        sets the new parameters of the HMM
        after one EM iteration
        
        pi K initial state distribution
        A KXK  state transition matrix
        alpha KXL gaussian priors
        means KxLxd means of the Gaussians
        cov KxLxdxd covarinaces of the Gaussians
        
        """
        #set pi
        self.pi = self.pi_Sum/self.rm_Sum
        #set A
        self.A = ((self.A_nom).T/self.A_den).T
        #set alpha
        self.alpha = ((self.alpha_Nom).T/self.A_den).T
        #set means
        self.set_means()
        #set_covariances
        self.set_covs()
        #set gauss
        self.set_gauss()
        
    def set_means( self ):
        """
        set the means after the EM update
        meaning--> after finding means_Nom
        also prepares the covarinaces fortheir calculation in the next step
        of setEm_updates
        
        """
        
        K = self.alpha_Nom.shape[0]
        L = self.alpha_Nom.shape[1]
        
        for k in np.arange( K ):
            self.means[k, :, :] = ((self.means_Nom[k, :, :]).T \
                                                    /self.alpha_Nom[k,:]).T
            for l in np.arange( L ):
                self.cov_Nom[k, l, :, :] = self.cov_Nom[k,l, :, :] \
                                                    /self.alpha_Nom[k,l]
        return
                      
    def set_covs( self ):
        """
        setting covariances 
        
        """
        
        K = self.alpha_Nom.shape[0]
        L = self.alpha_Nom.shape[1]
        
        for k in np.arange( K ):
            for l in np.arange( L ):
                meanKL = np.expand_dims( self.means[k,l,:], axis = 1).copy()
                self.cov[k, l, :, :] = self.cov_Nom[k,l,:,:] - meanKL@meanKL.T
            
        return
    
    def set_gauss( self ):
        """
        after having compute means and covariances on the EM step
        setting the gaussian objects
        
        """
        
        K = self.means.shape[0]
        L = self.means.shape[1]
        
        gaussStates = []
        for k in np.arange(K):
            gaussComponents = []
            for l in np.arange(L):
                gaussComponents.append( 
                        multivariate_normal(mean = self.means[k,l,:],
                                            cov = self.cov[k,l,:,:]) )
            gaussStates.append( gaussComponents )
            
        self.gauss = gaussStates
        return
                
            
            
        
            
            
        
    def update_pi(self,  gi1, rm_i):
        """ 
        updates the initial state parameter probabilities
        for all the states
        for the EM iteration
        given the values currently governed in the model
        p(z_1 = k|H = m)
        
        self.pi_Sum ---> the value to update
        
        USED IN EM_iter method
        """
        
        checkShape( self.pi_Sum, gi1, 'update_pi')
        self.pi_Sum += gi1*rm_i 
        
        return
        
        
    def update_A(self, xis_i, gamma_i,  rm_i):
        """
        updates the sum of for the EM iteration
        given the values currently governed in the model
        self.A_nom
        self.A_den
        
        Aij = p(z_t = j| z_(t-1) = i)
        """
        self.A_nom += np.sum( xis_i, axis = 2)*rm_i
        self.A_den += np.sum( gamma_i, axis = 1)*rm_i
        
        return 
        
        
        
    def update_alpha(self, g_i, gamma_i, rm_i):
        """
        updates the mixing gaussian coefficients for each state
        p(G_t = l| z_t = k) = prior probability for the lth component 
        of k state
        
        this eventually will be a matrix KxL
        
        g_i KXLXT
        gamma_i KxT
        rm_i = scalar
        
        """
        #get the number of states K
        K  = g_i.shape[0]
        L = g_i.shape[1]
        T = g_i.shape[2]
        
        #holding the memberships for the T samples of i observation
        # of L compunents of K states
        memb_i = np.zeros( shape = [K, L, T])
        #for each state compute the priors for the lth components
        for k in np.arange( K ):
            memb_i[k, :, :] =  g_i[k, :,:]*gamma_i[k, :]
            self.alpha_Nom[k, :] += np.sum( memb_i[k,:,:], axis = 1)
        
        self.alpha_Nom *= rm_i
        memb_i *= rm_i
        
        return memb_i
    
    
    def update_means_cov(self, X_i,  membs_i):
        """
        Updates the nominator  of the mean 
        vectors of the Gaussians and partially updates the nominator
        of of the covarinace matrices
        
        means_Nom KXLXd
        cov_Nom KXLXdXd
        
        X_i ith observation TXd
        membs_i memberships of the samples of observation x_i KXLXT
        
        """
        K = membs_i.shape[0]
        L = membs_i.shape[1]
       
        
        for k in np.arange(K):
            for l in np.arange( L ):
                X_iw = membs_i[k, l, :]*X_i.T #dxT
                self.means_Nom[k, l, :] += np.sum( X_iw, axis = 0)
                self.cov_Nom[k, l, :, :] +=  X_iw @ X_i #dxd sum on all T
                
        return
    
         
         
         


class MHMM(object):
    """
    
    Mixture of HMMs class using the HMM class
    
    """
    
    def __init__(self, n_HMMS = 2, n_states = 2, n_Comp = 2, EM_iter = 10,
                 t_cov = 'diag'):
        
        
        #setting number of states per HMM
        self.states = n_states
        #setting the number of HMMS attribute
        self.n_HMMS = n_HMMS
        #setting the number of components of HMM attribute
        self.n_Comp = n_Comp
        #setting the covarinace type attribute
        self.t_cov = t_cov
        #settint the number of EMiterations attribute
        self.Em_iter = EM_iter
        #initializing n_HMMs for our mixture
        self.HMMS = []
        self.init_HMMs()
        
        
    def init_HMMs( self ):
        """ 
        
        Initializing  M HMMs from the HMM class
        into the 
        HMMS list attribute 
        
        """
        
        M = self.n_HMMS
        states = self.states
        n_Comp = self.n_Comp
        t_cov = self.t_cov
        
        
        for m in np.arange( M ):
            self.HMMS.append( HMM( states = states, g_components = n_Comp,
                                  t_cov = t_cov))
            
        return self
        
        
        
        
    
    
    
    
    
    
    
    
    





