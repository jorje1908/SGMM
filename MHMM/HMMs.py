#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jul  9 14:08:17 2019

@author: george

Implements 
--> A hidden Markov Model and extends to
--> a Mixture of Hidden Markov Models,

--> Observational Model is consisted by a Mixture of 
Gaussians Model,

--> The Model will be trained by the Expectation Maximization
    Algorithm 


"""

import numpy as np
from scipy.stats import multivariate_normal

#########     check functions   #########
def checkShape(  arg1, arg2, name):
        
    if arg1.shape != arg2.shape:
        print( "Warning shapes does not match in " + name)
        
        
    return



class HMM(object):
    """ Helper HMM class """
    
    
    
    def __init__(self, states = 2, g_components = 2, t_cov = "diag"):
        
        #Setting states attribute
        self.states_ = states
        #Setting Gaussian components attribute
        self.g_components_ = g_components
        #initialize Transition Matrix
        self.A = np.zeros( shape = [states, states], dtype = 'float')
        #initilize initial distribution probabilities np.array pi[k] k = 1...K
        self.pi = np.zeros( shape = states, dtype = 'float')
        
        #4d array cov[k,l,d,d] kth state lth gaussian component
        self.cov = None
    
        #3d array means[k,l,d] kth state lth mean
        self.means = None
    
        #list of gaussian predictors gauss[k,l] lth gaussian of kth state
        #each gauss[k][l] component is an instance of the multivariate_normal
        #scipy object
        self.gauss = None
        #initial probabilities of each gaussian component, for each state
        #alpha[k,l] kth state lth component
        self.alpha = None
        
    def predict_states_All(self, X ):
        """
        computes the probability to observe a x_t for all t = 1...T
        for all states K = 1...K
        for any observation in time it calculates the probability 
        using the method predict_states
        
        X is a sequence of observations x1, ... xT
        numpy array Txd
        
        return the matrix P[k,t] probability to observe x_t given state k
        
        """
        
        #get the number of sequneces in time
        T = X.shape[0]
        #get number of states
        K = self.states_
        
        P_all = np.array( shape = [K, T])
        
        for t in np.arange( T ):
            P_all[:, t] = self.predict_states( X[t] )
        
        return P_all
        
    def predict_states(self, x):
        """ 
         calculates the probability of x to be observed given a state,
         for all states and returns a matrix 
         P(x) = [p(x_t|z_t = 1)... p(x_t|z_t = k)]
        
        """
        K = self.states_
        P = np.zeros( shape = [K] )
        
        for k in np.arange( K ):
            P[k] = self.predict_state( x, st = k)
            
        return P
        
        
    def predict_state(self, x, st = None):
         """
         calculates the probabiity of an observation to be generated 
         by the state "state" --> p(x_t | z_t = k)
         
         returns a matrix kx1
         
         """
         
         components = self.g_components_
         pk = 0
         
         for component in np.arange( components ):
             pk += self.predict_state_comp( x, st = st, cmp = component)
             
         return pk
            
         
         
    def predict_state_comp(self, x, st = None, cmp = None):
         
         """ 
         predicts the probability of an observation to be generated
         by the lth component gaussian of the kth state
         
         st: HMM  state {1...K}
         cmp: Kth state lth Gaussian Component l = {1...L}
         
         p( G_t = l, x_t |  z_t = k)
         
         """
         
         #get the multivariate_gaussian of state = "state"
         #component = "comp"
         gaussian_kl = self.gauss[st][ cmp ]
         #get the probability of this component to be chosen
         alpha_kl = self.alpha[st, cmp]
         #predict the probability of x to be generated by the lth component
         #of
         pkl = gaussian_kl.pdf( x )*alpha_kl
         
         return pkl
     
    def forward(self, X):
        """
        Implements the forward Algorithm, 
        Returns tha matrix forw [ a_1,... a_T ]
        where a_t = [at[1], ... ai[K]]^(transpose)
        where ai[k] = p( z_i = k, x_1...x_t)
        
        X is a sequence of observations x1, ... xT
        numpy array Txd
        
        """
       
        #get length of obseravtions in time
        T = X.shape[0]   
        #get number of states
        K = self.states_
        #get transition matrix Aij = p(z(t) = j|z(t-1) = i)
        A = self.A
        #initialize forward matrix
        forw = np.zeros( shape = [K, T])
        
        #take initial alphas a_1
        forw[:,0] = self.predict_states(X[0])*self.pi
        
        for t in np.arange(1, T):
            
            forw[:,t] = self.predict_states( X[t] )*(A.T@ forw[:, t-1] )
            
        return forw
    
    
    def backward(self, X):
        """
        Implements the Backward algorithm
        Returns the matrix backw [b_1,..., b_T]
        where b_t = [bt[1],..., bt[K]]
        where bt[k] = p(x_(t+1)...x_T| z_t = k)
        
        X is a sequence of observations x1, ... xT
        numpy array Txd
        
        """
        
        #get length of obseravtions in time
        T = X.shape[0]   
        #get number of states
        K = self.states_
        #get transition matrix Aij = p(z(t) = j|z(t-1) = i)
        A = self.A
        #initialize backward matrix
        backw = np.zeros( shape = [K, T])
        
        #initialize backw matrix at time T
        backw[:, T-1] = 1
        
        for t in np.arange( T-2, -1, -1):
            
            backw[:, t] = A.T@( self.predict_states( X[t+1]*backw[:,t+1]))
            
        return backw
    
    
    def gamas( self, X):
        
        """
        Computes th eprobability of being at state k at time t given
        we have observed all the sequence x1...xT,
        it is called  smoothed probability on hmms 
        p(z_t = k| O1...OT)
        
        X is a sequence of observations x1, ... xT
        numpy array Txd
        
        returns the gamma matrix  a KxT numpy array K are the HMM states
        T is the length of sequence in time
        
        """
        
        #regularization
        reg = 10**( -6 )
        
        #run forward algorithm_
        forw = self.forward( X )
        #run backward algorithm
        backw = self.backward( X )
        #calculate gamma unnormalized
        gamma = forw*backw 
        gammaSumCol = np.sum( gamma, axis = 0)
        #calculate final gamma
        gamma = gamma/( gammaSumCol + reg )
        
        return gamma


    def sliced( self, X):

        """
        Computes the sliced probability  p(z_t = i, Z_(t+1) = j| o1...T)
        
        returns a KxKxT-1 matrix  xis with the aformentioned probabilities
        
        """
        
        #get length of obseravtions in time
        T = X.shape[0]   
        #get number of states
        K = self.states_
        #get transition matrix Aij = p(z(t) = j|z(t-1) = i)
        A = self.A
        #initialize xis matrix
        xis = np.array( shape = [K, K, T-1 ])
        
        #compute observation proabilities for all time steps of X
        Pkt = self.predict_states_All( X )
        #compute forward
        forw = self.forward( X )
        #compute backward 
        backw = self.backward( X )
        
        for t in np.arange( T-1 ):
            
            xis[:, :, t] = (A.T*forw[:, t]).T*( Pkt[:, t+1]*backw[:, t+1] )
            xisSum = np.sum( xis[:, :, t])
            xis[:,:,t] = xis[:,:,t]/xisSum
            
        return xis
    
    def g(self, x_i):
        """
        calculates the posterior probabilities 
        
        p(G = l|x_t, H = m, z_t = k)
        for all l = {1,...,L}
                t = {1,...,T}
                z_t = {1,...K}
                
        return an KXLXT matrix containing the gkl(t) of the derivation
        x_i is a Txd array having the observations in time
        """
        #get the time up to observations reach T
        T = x_i.shape[0]
        #get the number of states
        states = self.states_
        #get the number of gaussian components
        L = self.g_components_
        #initialize matrix
        gs = np.zeros( shape = [states, L, T] )
        
        for state in np.arange( states ):
            gs[state, :, :] = self.g_state( x_i, state )
            
        return gs
            
    
    def g_state(self, x_i, st = None):
        """
        computes the probability p(G_t = l|z_t = k, x_t)
        for state z_t = st and component G_t = comp
        for each time observation o_t t=1 ...T 
        for each component
        thus returning a :
        
        LxT matrix
        
        x_i = [x_i(1),...x_i(T)]
        """
        #number of gaussian components
        gauss_comp = self.g_components_
        #number of observations in time
        T = x_i.shape[0]
        #initialize the matrix to return
        pk = np.zeros( shape = [gauss_comp, T])
        
        for cmp in np.arange( gauss_comp ):
            pk[cmp, :] = self.predict_state_comp(x_i, st = st, cmp = cmp)
        
        #get the component wise sum for each sample in time
        sumPk = np.sum( pk, axis = 0)
        #normalize the pk matrix such that every column sums to 0
        pk = pk/sumPk
        return pk
        
    
    def EM_init(self, X):
        """
        Initialize the HMM parameters
        """
        pass
    
    def EM_iter(self, X, r_m):
        """ 
        
        EM iteration updating all the 
        HMM parameteres
        
        A: state transition matrix
        pi: initial state probabilities
        alpha: gaussian mixing coefficients [k,l] matrix
        means: Gaussian Means [k,l,d] matrix
        cov: Coavriance matrices for the Gaussians [k,l,d,d]
        
        sets the A,pi, alpha, means, cov
        and gauss: gaussian components objects (frozen)
        
        X: is a list of obseravtions O^(1)... O^(N)
        O^(i) = [o_1^(i),....o_T^(i)]
        
        r_m: N dimensional matrix with the posterior probability p(H = m|X)
        which the probability of this HMM to be chosen given the observations
        X
        
        """
        
        K = self.states_
        L = self.g_components_
        
        #feature dimension
        d = X.shape[1]
       
        #initializing attributes needed
        
        #sum  of initial state distributions
        #for all the K states
        self.pi_Sum = np.zeros( shape = [K] )
        
        #sum of all rm_i's
        self.rm_Sum = np.sum(r_m)
        
        #nominator for state transition probabilities
        self.A_nom = np.zeros( shape = [K,K] )
        
        #A denominator is the same as alpha denominator
        self.A_den = np.zeros( shape = [K] )
        
        #alpha_Nom is the same as means denominator
        #and covarinaces denominator
        #priors of the Gaussian components
        self.alpha_Nom = np.zeros( shape = [K, L] )
        
        #nominator of the means
        self.means_Nom = np.zeros( shape = [K, L, d])
        #nominator of the covariances #need to subtract means
        self.cov_Nom = np.zeros( shape = [K, L, d, d] )
        
        for i in np.arange( len(X) ):
            #get the ith observation
            x_i = X[i]
            #get the gammas for the i_th observation KXT
            gamma_i = self.gamas( x_i )
            #get xis for the i_th observation KXKX(T-1)
            xis_i = self.sliced( x_i )
            #get the rm_i,  N 
            rm_i = r_m[i]
            #get g_is for the ith observation KxLXT
            g_i = self.gs( x_i )
            
            #update sum of pis
            self.update_pi( gamma_i[:, 0], rm_i )
            #update A matrix nominator and denominator
            self.update_A( xis_i, gamma_i, rm_i )
            #update alphas
            membs_i=self.update_alpha( rm_i, gamma_i, g_i)
            
            self.update_means_cov( x_i, membs_i)
            
        
    def setEm_updates(self ) :
        """
        sets the new parameters of the HMM
        after one EM iteration
        
        pi K initial state distribution
        A KXK  state transition matrix
        alpha KXL gaussian priors
        means KxLxd means of the Gaussians
        cov KxLxdxd covarinaces of the Gaussians
        
        """
        #set pi
        self.pi = self.pi_Sum
        #set A
        self.A = ((self.A_nom).T/self.A_den).T
        #set alpha
        self.alpha = ((self.alpha_Nom).T/self.A_den).T
        #set means
        self.set_means()
        #set_covariances
        self.set_covs()
        #set gauss
        self.set_gauss()
        
    def set_means( self ):
        """
        set the means after the EM update
        meaning--> after finding means_Nom
        also prepares the covarinaces fortheir calculation in the next step
        of setEm_updates
        
        """
        
        K = self.alpha_Nom.shape[0]
        L = self.alpha_Nom.shape[1]
        
        for k in np.arange( K ):
            self.means[k, :, :] = ((self.means_Nom[k, :, :]).T \
                                                    /self.alpha_Nom[k,:]).T
            for l in np.arange( L ):
                self.cov_Nom[k, l, :, :] = self.cov_Nom[k,l, :, :] \
                                                    /self.alpha_Nom[k,l]
        return
                      
    def set_covs( self ):
        """
        setting covariances 
        
        """
        
        K = self.alpha_Nom.shape[0]
        L = self.alpha_Nom.shape[1]
        
        for k in np.arange( K ):
            for l in np.arange( L ):
                meanKL = np.expand_dims( self.means[k,l,:], axis = 1).copy()
                self.cov[k, l, :, :] = self.cov_Nom[k,l,:,:] - meanKL@meanKL.T
            
        return
    
    def set_gauss( self ):
        """
        after having compute means and covariances on the EM step
        setting the gaussian objects
        
        """
        
        K = self.means.shape[0]
        L = self.means.shape[1]
        
        gaussStates = []
        for k in np.arange(K):
            gaussComponents = []
            for l in np.arange(L):
                gaussComponents.append( 
                        multivariate_normal(mean = self.means[k,l,:],
                                            cov = self.cov[k,l,:,:]) )
            gaussStates.append( gaussComponents )
            
        self.gauss = gaussStates
        return
                
            
            
        
            
            
        
    def update_pi(self,  gi1, rm_i):
        """ 
        updates the initial state parameter probabilities
        for all the states
        for the EM iteration
        given the values currently governed in the model
        p(z_1 = k|H = m)
        
        self.pi_Sum ---> the value to update
        
        USED IN EM_iter method
        """
        
        checkShape( self.pi_Sum, gi1, 'update_pi')
        self.pi_Sum += gi1*rm_i / (self.rm_Sum)
        
        return
        
        
    def update_A(self, xis_i, gamma_i,  rm_i):
        """
        updates the sum of for the EM iteration
        given the values currently governed in the model
        self.A_nom
        self.A_den
        
        Aij = p(z_t = j| z_(t-1) = i)
        """
        self.A_nom += np.sum( xis_i, axis = 2)*rm_i
        self.A_den += np.sum( gamma_i, axis = 1)*rm_i
        
        return 
        
        
        
    def update_alpha(self, g_i, gamma_i, rm_i):
        """
        updates the mixing gaussian coefficients for each state
        p(G_t = l| z_t = k) = prior probability for the lth component 
        of k state
        
        this eventually will be a matrix KxL
        
        g_i KXLXT
        gamma_i KxT
        rm_i = scalar
        
        """
        #get the number of states K
        K  = g_i.shape[0]
        L = g_i.shape[1]
        T = g_i.shape[2]
        
        #holding the memberships for the T samples of i observation
        # of L compunents of K states
        memb_i = np.zeros( shape = [K, L, T])
        #for each state compute the priors for the lth components
        for k in np.arange( K ):
            memb_i[k, :, :] =  g_i[k, :,:]*gamma_i[k, :]
            self.alpha_Nom[k, :] += np.sum(memb_i[k,:,:], axis = 1)
        
        self.alpha_Nom *= rm_i
        memb_i *= rm_i
        
        return memb_i
    
    
    def update_means_cov(self, X_i,  membs_i):
        """
        Updates the nominator  of the mean 
        vectors of the Gaussians and partially updates the nominator
        of of the covarinace matrices
        
        means_Nom KXLXd
        cov_Nom KXLXdXd
        
        X_i ith observation TXd
        membs_i memberships of the samples of observation x_i KXLXT
        
        """
        K = membs_i.shape[0]
        L = membs_i.shape[1]
       
        
        for k in np.arange(K):
            for l in np.arange( L ):
                X_iw = membs_i[k, l, :]*X_i.T #dxT
                self.means_Nom[k, l, :] += np.sum( X_iw, axis = 0)
                self.cov_Nom[k, l, :, :] +=  X_iw @ X_i #dxd sum on all T
                
        return
    
    
    
    
   
    
    
    
    
    
   
        
    
    
    
    
        
        
            
        
        
        
        
        
        
        
        
        
        
        
        
        
        
         
         
         
         
         


class MHMM(object):
    pass





