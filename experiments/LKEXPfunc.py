#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Sat Jun  8 23:23:35 2019

@author: george
"""

import numpy as np
import matplotlib.pyplot as plt
import scipy as sc
from scipy.stats import multivariate_normal
from supervisedGmm import SupervisedGMM
from metricsFunctions import optimalTau, calc_metrics, metrics_cluster, sgmmResults
import time
import pandas as pd
from dataGen import   genData1D, bayesO, MC, bayesOF, MCF,\
probxG, probyG, genxG, genyG, genYG, sigm, genXG, gauss

from sklearn.linear_model import LogisticRegression


def assignLabel( data ):
    
    """TAKEN DATA GENERATED BY genData1D from dataGen.py
    it separates them according to their label """
    
    labels = data[:, 2]
    pos = np.where( labels == 1 )[0]
    neg = np.where( labels == 0 )[0]
    
    Xpos = data[pos]
    Xneg = data[neg]
    
    return Xpos, Xneg


def assignGen( data ):
    
    """TAKEN DATA GENERATED BY genData1D from dataGen.py
    it separates them according to the gaussian they were generated 
    from"""
    
    genLabels = data[:, 3]
    g0 = np.where( genLabels == 0 )[0]
    g1 = np.where( genLabels == 1 )[0]
    
    X0 = data[ g0 ]
    X1 = data[ g1 ]
    
    return X0, X1


def calcStats( data, Nall ):
    """ Takes a portion of the data  and generated
    their statistics assuming they were generated by a gaussian
    """
    
    pi = data.shape[0]/Nall
    mean = np.mean(data)
    std = np.std( data )
    
    return pi , mean, std

def logisTrain(Xc, Yc, ite = 5000000, printO = 0 ):
    """ Takes Xc = Xtrain and their Labels = Yc it calculates the optimal 
    logistic regression classifier """
    
    w = np.random.rand(2)
    w1 = w
    lr = 0.09
    objOld = 10000000
    reg = 10**(-10 )
    
    ones = np.ones(shape = [Xc.shape[0], 1] )
    Xcc = np.expand_dims(Xc.copy(), axis = 1 )
    X = np.concatenate((ones, Xcc), axis = 1 )
    Y = np.expand_dims( Yc.copy(), axis = 1 )
    second = X.T@(-Y)
    N = Xc.shape[0]
    i = 0
    al = 0.8
    be = 1.01
    m = 0.9*0
  #  print("\n")
    w0 = np.zeros(2)
    while True:
        s = np.expand_dims( sigm(Xc, w), axis = 1 )
        H = X.T@( np.diag(np.squeeze(s*(1-s), axis = 1) ) )@X
        t1 = Y*np.log( s + reg)
       # print(s.shape, Y.shape, t1.shape)
        t2 = (1-Y)*np.log(1-s +reg)
        t3 = -(t1 + t2)
        obj = np.sum(t3)/N
        z =  X.T@(s)
        der =  np.squeeze(z + second, axis = 1)
        
        w2 = w1 - lr*np.linalg.pinv(H)@der + m*( w1 - w0)
        #w2 = w1 - lr*der + m*( w1 - w0)
        
        w0 = w1
        w1 = w2
        w = w2
        
        if printO == 1:
            print("iter: {}, obj:{:0.10f}, lr:{:0.10f}".format(i, obj, lr) )
            #print("i: {}, der0:{}, der1:{}".format(i, der[0], der[1]) )
#            
#        if abs(obj - objOld) < 10**(-10) :
#            break
            
        if abs(np.sum(der) ) /2 < 10**(-6):
            break
        
        if obj > objOld:
             #print("Diverges at iter {}".format(i))
             lr = lr*al
        if obj - objOld < -10**(-2):
#           print("Diverges at iter {}".format(i))
            lr = lr*be
      #  if lr < 0.00001:
           # lr = 0.00001
      
        objOld = obj
        i += 1
    
    
    py = sigm(Xc, w)*Yc + (1- sigm(Xc, w))*(1-Yc)
    return w, py

def logisTrainScikit(Xc, Yc ):
    
    logis = LogisticRegression(penalty = 'l2', random_state = 0,
                               solver = 'newton-cg', C = 10**(100))
    model = logis.fit(np.expand_dims(Xc, axis = 1), Yc)
    w1 = model.coef_[0,0]
    w0 = model.intercept_[0]
    w = [w0, w1]
    
    py = model.predict_proba(np.expand_dims(Xc, axis = 1))[ np.arange(Yc.shape[0]), Yc.astype(int) ]
    
    return w, py


def Qfunc(X, Y, Nall, printO = 0):
    
    """ Calculate the Q auxilary function ,
     take the data and the labels
     calculate the statistics for data likelihood
     fitting the logistic regresiion model
     and then calculate the labels log likelihood
     and take the sum of them"""
     
    pi, mean, std = calcStats( X, Nall )
    
    w,_ = logisTrain(X, Y)
    
    lx = np.log(pi*gauss(X, mean, std) + 10**(-10) )
    ly = np.log( sigm(X, w)*Y + (1-sigm(X, w))*(1-Y) + 10**(-10))
    L = np.sum(lx) + np.sum(ly)
    stats = [pi, mean, std, w]
    return L, lx, ly, stats

def QfuncScikit(X, Y, Nall, printO = 0):
    
    """ Calculate the Q auxilary function ,
     take the data and the labels
     calculate the statistics for data likelihood
     fitting the logistic regresiion model
     and then calculate the labels log likelihood
     and take the sum of them"""
     
    pi, mean, std = calcStats( X, Nall )
    w,_ = logisTrain(X, Y)
    
    lx = np.log(pi*gauss(X, mean, std) + 10**(-10) )
    ly = np.log( sigm(X, w)*Y + (1-sigm(X, w))*(1-Y) + 10**(-10))
    L = np.sum(lx) + np.sum(ly)
    stats = [pi, mean, std, w]
    return L, lx, ly, stats

def QfuncAll(X1, X2, Y1, Y2, Nall, printO = 0):
    """ TAkes two datasets and calculates the likelihoods and
    q functions of them,
    returns in addition some statistics
    """
    
    L1, lx1, ly1, s1 = Qfunc( X1, Y1, Nall)
    L2, lx2, ly2, s2 = Qfunc( X2, Y2, Nall)
    Lall = L1 + L2
    
    myd = {"lx1":lx1, "lx2":lx2, "ly1":ly1, "ly2": ly2,"stats1":s1,
           "stats2": s2}
    Like = likelihood(X1, X2, Y1, Y2, s1, s2, Nall)
    
    return Lall/Nall, myd, Like

def likelihood( X1, X2, Y1, Y2,  s1, s2, Nall ):
    p1, p2 = s1[0], s2[0]
    m1, m2 = s1[1], s2[1]
    std1, std2 = s1[2], s2[2]
    w1, w2 = s1[3], s2[3]
    
    PX1 = p1*gauss(X1, m1, std1) + p2*gauss(X1, m2, std2)
    PX2 = p1*gauss(X2, m1, std1) + p2*gauss(X2, m2, std2)
    PY1 = sigm(X1, w1)*Y1 + (1-sigm(X1, w1))*(1-Y1)
    PY11 = PY1 + sigm(X1, w2)*Y1 + (1-sigm(X1, w2))*(1-Y1)
    PY2 = sigm(X2, w1)*Y2 + (1-sigm(X2, w1))*(1-Y2)
    PY22 = PY2 + sigm(X2, w2)*Y2 + (1-sigm(X2, w2))*(1-Y2)
    
    L1 = np.sum( np.log( PX1*PY11 +10**(-10)) )
    L2 = np.sum(np.log( PX2*PY22 +10**(-10)) )
    Like = (L1 + L2)/Nall
    return Like


def LKEXP(N, batches, start, enhance, step, wone, wtwo, pis, meanX, covX,
          meanY, covY):

    N = N
    batches  = batches

    L1l = []
    L2l = []
    L1lk = []
    L2lk = []

    for enh in np.arange(start, enhance, step):
    
        w1 = np.array( wone )*enh
        w2 = np.array( wtwo )*enh
        w = [w1, w2]
    
        L1all = 0
        L2all = 0
        L1allLk = 0
        L2allLk = 0
    
        for batch in np.arange( batches ):
            print( "Enh: {}, batch: {}".format(enh, batch))

            data = genData1D( pis, meanX, covX,  meanY,  covY, w, N )


            dpos, dneg = assignLabel( data )
            d1, d2 = assignGen( data )

            pPos, mPos, stdPos = calcStats( dpos[:, 1], N)
            pNeg, mNeg, stdNeg = calcStats( dneg[:, 1], N)

            pPos1, mPos1, stdPos1 = calcStats( d1[:, 1], N)
            pNeg2, mNeg2, stdNeg2 = calcStats( d2[:, 1], N)
    



            L1, myDict, Li1 = QfuncAll( dpos[:,1], dneg[:,1], dpos[:,2], 
                                                   dneg[:,2], data.shape[0])

            L2, myDict2, Li2 = QfuncAll( d1[:,1], d2[:,1], d1[:,2],
                                                    d2[:,2], data.shape[0])
        
            L1all += L1
            L2all += L2
            L1allLk += Li1
            L2allLk += Li2
    
        L1l.append( L1all/batches )
        L2l.append( L2all/batches )
        L1lk.append( L1allLk/batches )
        L2lk.append( L2allLk/batches )




    index = np.arange( len(L1l ) )
    
    return L1l, L2l, L1lk, L2lk, index





